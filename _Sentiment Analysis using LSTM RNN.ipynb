{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" Sentiment Analysis using LSTM RNN.ipynb","provenance":[{"file_id":"19wZi7P0Tzq9ZxeMz5EDmzfWFBLFWe6kN","timestamp":1622734555222}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GYiRsFGD6iUC"},"source":["# 0 TorchText"]},{"cell_type":"markdown","metadata":{"id":"tp5IzBGsPGHs"},"source":["## Dataset Preview\n","\n","Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n","\n","We will be using previous session tweet dataset. Let's just preview the dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bY6Ma0aIpe6p","executionInfo":{"status":"ok","timestamp":1622755448134,"user_tz":-330,"elapsed":2706,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"4afdc7eb-33e9-46c3-dca7-e268f2795050"},"source":["!pip3 install pickle5\n","import pandas as pd\n","import pickle5 as pickle\n","f = open(\"data_aug_random_deletion.pkl\",\"rb\")\n","df = pickle.load(f)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"JcFzLw7Wqo1j","executionInfo":{"status":"ok","timestamp":1622755448135,"user_tz":-330,"elapsed":11,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"33d3e311-8ee4-462f-b6c0-4c94395e6d48"},"source":["df"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Rock is destined to be the 21st Century 's...</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The gorgeously elaborate continuation of `` Th...</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Effective but too-tepid biopic</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>If you sometimes like to go to the movies to h...</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Emerges as something rare , an issue movie tha...</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20220</th>\n","      <td>ever in DiCaprio 's is performance best anythi...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>20221</th>\n","      <td>that as and telling story that examines forbid...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>20222</th>\n","      <td>Jeffrey the 's performance exterminator Tambor...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>20223</th>\n","      <td>Twenty remains years its first release , E.T. ...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>20224</th>\n","      <td>root the fictional . most examination of cause...</td>\n","      <td>24</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20225 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                sentence  labels\n","0      The Rock is destined to be the 21st Century 's...      16\n","1      The gorgeously elaborate continuation of `` Th...      19\n","2                         Effective but too-tepid biopic      12\n","3      If you sometimes like to go to the movies to h...      17\n","4      Emerges as something rare , an issue movie tha...      20\n","...                                                  ...     ...\n","20220  ever in DiCaprio 's is performance best anythi...      24\n","20221  that as and telling story that examines forbid...      24\n","20222  Jeffrey the 's performance exterminator Tambor...      24\n","20223  Twenty remains years its first release , E.T. ...      24\n","20224  root the fictional . most examination of cause...      24\n","\n","[20225 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"R7JdpCW-YbAG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622755449511,"user_tz":-330,"elapsed":667,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"1c62fe4d-01a2-4195-f5b9-de4a61de4612"},"source":["df.shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20225, 2)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"zqRsoF6xYdgl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622755450189,"user_tz":-330,"elapsed":7,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"3fee4267-4010-4c7c-c7a0-e8f786a169be"},"source":["df.labels.value_counts()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19    953\n","7     853\n","16    819\n","15    800\n","21    800\n","1     800\n","17    800\n","2     800\n","18    800\n","3     800\n","4     800\n","20    800\n","5     800\n","6     800\n","14    800\n","22    800\n","23    800\n","8     800\n","24    800\n","9     800\n","10    800\n","11    800\n","12    800\n","13    800\n","0     800\n","Name: labels, dtype: int64"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"XJ6o_79ISSVb"},"source":["## Defining Fields"]},{"cell_type":"markdown","metadata":{"id":"e63g08ijOrf7"},"source":["Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequen tial to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."]},{"cell_type":"code","metadata":{"id":"qk8IP4SK1Lrp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622755452249,"user_tz":-330,"elapsed":5,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"dd794db0-7ad2-462f-ee98-e1f6c6e7ff4f"},"source":["# Import Library\n","import random\n","import torch, torchtext\n","from torchtext import data \n","\n","# Manual Seed\n","SEED = 43\n","torch.manual_seed(SEED)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fb6fb3ca290>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"u6bKQax2Mf_U","executionInfo":{"status":"ok","timestamp":1622755455935,"user_tz":-330,"elapsed":1999,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["Sentence = torchtext.legacy.data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n","Label = torchtext.legacy.data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2nfZ3bG7IAQ"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"mX-lYIe_O7Vy"},"source":["Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"]},{"cell_type":"code","metadata":{"id":"VawdWq36O6td","executionInfo":{"status":"ok","timestamp":1622755455937,"user_tz":-330,"elapsed":3,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["fields = [('sentence', Sentence),('labels',Label)]"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tc92trR-O7GB","executionInfo":{"status":"ok","timestamp":1622752703479,"user_tz":-330,"elapsed":6,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"37b5f371-7368-45ea-9d70-c42e62fcca5c"},"source":["df.iloc[11289]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sentence    ... the story is far-flung , illogical , and p...\n","labels                                                      0\n","Name: 11289, dtype: object"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"ZbtZ-Ph2P1xL"},"source":["Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."]},{"cell_type":"code","metadata":{"id":"L3OLcJ5B7rHz","executionInfo":{"status":"ok","timestamp":1622756009392,"user_tz":-330,"elapsed":551819,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["example = [torchtext.legacy.data.Example.fromlist([df.sentence[i],df.labels[i]], fields) for i in range(df.shape[0])] "],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"nT-flpH-P1cd","executionInfo":{"status":"ok","timestamp":1622756009393,"user_tz":-330,"elapsed":29,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["# Creating dataset\n","#twitterDataset = data.TabularDataset(path=\"tweets.csv\", format=\"CSV\", fields=fields, skip_header=True)\n","\n","stanfordDataset = torchtext.legacy.data.Dataset(example, fields)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6ZnyCPaR08F"},"source":["Finally, we can split into training, testing, and validation sets by using the split() method:"]},{"cell_type":"code","metadata":{"id":"uPYXyuKhRpBk","executionInfo":{"status":"ok","timestamp":1622756009393,"user_tz":-330,"elapsed":28,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["(train, valid) = stanfordDataset.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gpmKkoIO8vEO"},"source":[""]},{"cell_type":"code","metadata":{"id":"ykvsCGQMR6UD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622756009394,"user_tz":-330,"elapsed":28,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"67229718-cb4d-40a8-a5da-2f9d98802ab7"},"source":["(len(train), len(valid))"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(17191, 3034)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"kix8P2IKSBaV"},"source":["An example from the dataset:"]},{"cell_type":"code","metadata":{"id":"dUpEOQruR9JL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622756009394,"user_tz":-330,"elapsed":11,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"050689f2-dd2a-4091-c40b-3b4892138ca5"},"source":["vars(train.examples[100])"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'labels': 8,\n"," 'sentence': ['A',\n","  'sophomoric',\n","  'exploration',\n","  'of',\n","  '`',\n","  'life',\n","  'problems',\n","  \"'\",\n","  'most',\n","  'people',\n","  'solved',\n","  'long',\n","  'ago',\n","  '--',\n","  'or',\n","  'at',\n","  'least',\n","  'got',\n","  'tired',\n","  'of',\n","  'hearing',\n","  'people',\n","  'kvetch',\n","  'about',\n","  '.']}"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"AKdllP3FST4N"},"source":["## Building Vocabulary"]},{"cell_type":"markdown","metadata":{"id":"SuvWQ-SpSmSz"},"source":["At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n","\n","Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"]},{"cell_type":"code","metadata":{"id":"mx955u93SGeY","executionInfo":{"status":"ok","timestamp":1622756009394,"user_tz":-330,"elapsed":7,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["Sentence.build_vocab(train)\n","Label.build_vocab(train)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvyEeEjXTGhX"},"source":["By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."]},{"cell_type":"code","metadata":{"id":"rA3tIESdcJdN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622753185812,"user_tz":-330,"elapsed":24,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"43b4456f-8309-4f46-ff2c-52f068a84006"},"source":["print('Size of input vocab : ', len(Sentence.vocab))\n","print('Size of label vocab : ', len(Label.vocab))\n","print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n","print('Labels : ', Label.vocab.stoi)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Size of input vocab :  18400\n","Size of label vocab :  25\n","Top 10 words appreared repeatedly : [('.', 15990), (',', 14263), ('the', 12036), ('of', 9030), ('and', 8574), ('a', 8422), ('to', 5736), ('-', 5251), ('is', 4925), (\"'s\", 4847)]\n","Labels :  defaultdict(None, {19: 0, 7: 1, 11: 2, 1: 3, 4: 4, 16: 5, 17: 6, 0: 7, 9: 8, 15: 9, 21: 10, 5: 11, 6: 12, 2: 13, 14: 14, 24: 15, 10: 16, 20: 17, 8: 18, 23: 19, 3: 20, 13: 21, 22: 22, 12: 23, 18: 24})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rwjD2-ebTeUX"},"source":["**Lots of stopwords!!**"]},{"cell_type":"markdown","metadata":{"id":"sLWW221gTpNs"},"source":["Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."]},{"cell_type":"markdown","metadata":{"id":"EQqMhMoDUDmn"},"source":["But at first declare the device we are using."]},{"cell_type":"code","metadata":{"id":"Zfo2QhGJUK4l","executionInfo":{"status":"ok","timestamp":1622753185812,"user_tz":-330,"elapsed":20,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"zK2ORoqdTNsM","executionInfo":{"status":"ok","timestamp":1622753185813,"user_tz":-330,"elapsed":20,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["train_iterator, valid_iterator = torchtext.legacy.data.BucketIterator.splits((train, valid), batch_size = 32, \n","                                                            sort_key = lambda x: len(x.sentence),\n","                                                            sort_within_batch=True, device = device)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gg7gTFQO4fby"},"source":["Save the vocabulary for later use"]},{"cell_type":"code","metadata":{"id":"niE9Cc6-2bD_","executionInfo":{"status":"ok","timestamp":1622753185813,"user_tz":-330,"elapsed":19,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["import os, pickle\n","with open('tokenizer.pkl', 'wb') as tokens: \n","    pickle.dump(Sentence.vocab.stoi, tokens)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1AbsQwqkVyAy"},"source":["## Defining Our Model"]},{"cell_type":"markdown","metadata":{"id":"E4PED4HJWH4t"},"source":["We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n","\n","In this model we create three layers. \n","1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n","2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n","3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."]},{"cell_type":"code","metadata":{"id":"43pVRccMT0bT","executionInfo":{"status":"ok","timestamp":1622753185814,"user_tz":-330,"elapsed":19,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class classifier(nn.Module):\n","    \n","    # Define all the layers used in model\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n","        \n","        super().__init__()          \n","        \n","        # Embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        \n","        # LSTM layer\n","        self.encoder = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           dropout=dropout,\n","                           batch_first=True)\n","        # try using nn.GRU or nn.RNN here and compare their performances\n","        # try bidirectional and compare their performances\n","        \n","        # Dense layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        \n","    def forward(self, text, text_lengths):\n","        \n","        # text = [batch size, sent_length]\n","        embedded = self.embedding(text)\n","        # embedded = [batch size, sent_len, emb dim]\n","      \n","        # packed sequence\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n","        \n","        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n","        #hidden = [batch size, num layers * num directions,hid dim]\n","        #cell = [batch size, num layers * num directions,hid dim]\n","    \n","        # Hidden = [batch size, hid dim * num directions]\n","        dense_outputs = self.fc(hidden)   \n","        \n","        # Final activation function softmax\n","        output = F.softmax(dense_outputs[0], dim=1)\n","            \n","        return output"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwBoGE_X_Fl8","executionInfo":{"status":"ok","timestamp":1622753185814,"user_tz":-330,"elapsed":19,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["# Define hyperparameters\n","size_of_vocab = len(Sentence.vocab)\n","embedding_dim = 300\n","num_hidden_nodes = 100\n","num_output_nodes = 25\n","num_layers = 2\n","dropout = 0.2\n","\n","# Instantiate the model\n","model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-pOMqzJ3eTv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622753185815,"user_tz":-330,"elapsed":17,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"c295e2ea-76da-4689-94df-caa84292bf73"},"source":["print(model)\n","\n","#No. of trianable parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    \n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["classifier(\n","  (embedding): Embedding(18400, 300)\n","  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n","  (fc): Linear(in_features=100, out_features=25, bias=True)\n",")\n","The model has 5,764,125 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eXajorf5Xz7t"},"source":["## Model Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"PrE9RpMtZ1Vs"},"source":["First define the optimizer and loss functions"]},{"cell_type":"code","metadata":{"id":"-u86JWdlXvu5","executionInfo":{"status":"ok","timestamp":1622753189098,"user_tz":-330,"elapsed":3296,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["import torch.optim as optim\n","\n","# define optimizer and loss\n","optimizer = optim.Adam(model.parameters(), lr=2e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","# define metric\n","def binary_accuracy(preds, y):\n","    #round predictions to the closest integer\n","    _, predictions = torch.max(preds, 1)\n","    \n","    correct = (predictions == y).float() \n","    acc = correct.sum() / len(correct)\n","    return acc\n","    \n","# push to cuda if available\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VCJtNb3Zt8w"},"source":["The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."]},{"cell_type":"markdown","metadata":{"id":"2WjEPLKsAiS_"},"source":["**Training Loop**"]},{"cell_type":"code","metadata":{"id":"HDWNnGK3Y5oJ","executionInfo":{"status":"ok","timestamp":1622753189102,"user_tz":-330,"elapsed":11,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    # initialize every epoch \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    # set the model in training phase\n","    model.train()  \n","    \n","    for batch in iterator:\n","        \n","        # resets the gradients after every batch\n","        optimizer.zero_grad()   \n","        \n","        # retrieve text and no. of words\n","        tweet, tweet_lengths = batch.sentence   \n","        \n","        # convert to 1D tensor\n","        predictions = model(tweet, tweet_lengths).squeeze()  \n","        \n","        # compute the loss\n","        loss = criterion(predictions, batch.labels)        \n","        \n","        # compute the binary accuracy\n","        acc = binary_accuracy(predictions, batch.labels)   \n","        \n","        # backpropage the loss and compute the gradients\n","        loss.backward()       \n","        \n","        # update the weights\n","        optimizer.step()      \n","        \n","        # loss and accuracy\n","        epoch_loss += loss.item()  \n","        epoch_acc += acc.item()    \n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CZcHhkkvAsCt"},"source":["**Evaluation Loop**"]},{"cell_type":"code","metadata":{"id":"zHEe-zSVAriL","executionInfo":{"status":"ok","timestamp":1622753189103,"user_tz":-330,"elapsed":8,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    # initialize every epoch\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    # deactivating dropout layers\n","    model.eval()\n","    \n","    # deactivates autograd\n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","        \n","            # retrieve text and no. of words\n","            tweet, tweet_lengths = batch.sentence\n","            \n","            # convert to 1d tensor\n","            predictions = model(tweet, tweet_lengths).squeeze()\n","            \n","            # compute loss and accuracy\n","            loss = criterion(predictions, batch.labels)\n","            acc = binary_accuracy(predictions, batch.labels)\n","            \n","            # keep track of loss and accuracy\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6LJFW7HaJoV"},"source":["**Let's Train and Evaluate**"]},{"cell_type":"code","metadata":{"id":"tq330XlnaEU9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622753534080,"user_tz":-330,"elapsed":164213,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"b6100a3c-dfc9-4b27-bb40-a0d52991ed23"},"source":["N_EPOCHS = 50\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","     \n","    # train the model\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    \n","    # evaluate the model\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["\tTrain Loss: 2.449 | Train Acc: 84.03%\n","\t Val. Loss: 2.783 |  Val. Acc: 50.71% \n","\n","\tTrain Loss: 2.445 | Train Acc: 84.39%\n","\t Val. Loss: 2.781 |  Val. Acc: 50.68% \n","\n","\tTrain Loss: 2.442 | Train Acc: 84.61%\n","\t Val. Loss: 2.783 |  Val. Acc: 50.39% \n","\n","\tTrain Loss: 2.439 | Train Acc: 84.87%\n","\t Val. Loss: 2.781 |  Val. Acc: 50.74% \n","\n","\tTrain Loss: 2.435 | Train Acc: 85.22%\n","\t Val. Loss: 2.780 |  Val. Acc: 50.55% \n","\n","\tTrain Loss: 2.429 | Train Acc: 86.08%\n","\t Val. Loss: 2.782 |  Val. Acc: 50.42% \n","\n","\tTrain Loss: 2.423 | Train Acc: 86.67%\n","\t Val. Loss: 2.784 |  Val. Acc: 50.05% \n","\n","\tTrain Loss: 2.418 | Train Acc: 87.20%\n","\t Val. Loss: 2.780 |  Val. Acc: 50.65% \n","\n","\tTrain Loss: 2.413 | Train Acc: 87.62%\n","\t Val. Loss: 2.781 |  Val. Acc: 50.50% \n","\n","\tTrain Loss: 2.410 | Train Acc: 87.85%\n","\t Val. Loss: 2.779 |  Val. Acc: 50.82% \n","\n","\tTrain Loss: 2.406 | Train Acc: 88.17%\n","\t Val. Loss: 2.780 |  Val. Acc: 50.51% \n","\n","\tTrain Loss: 2.403 | Train Acc: 88.35%\n","\t Val. Loss: 2.779 |  Val. Acc: 50.82% \n","\n","\tTrain Loss: 2.402 | Train Acc: 88.44%\n","\t Val. Loss: 2.777 |  Val. Acc: 50.70% \n","\n","\tTrain Loss: 2.400 | Train Acc: 88.59%\n","\t Val. Loss: 2.782 |  Val. Acc: 50.18% \n","\n","\tTrain Loss: 2.399 | Train Acc: 88.74%\n","\t Val. Loss: 2.780 |  Val. Acc: 50.45% \n","\n","\tTrain Loss: 2.396 | Train Acc: 88.99%\n","\t Val. Loss: 2.779 |  Val. Acc: 50.61% \n","\n","\tTrain Loss: 2.396 | Train Acc: 89.02%\n","\t Val. Loss: 2.776 |  Val. Acc: 51.01% \n","\n","\tTrain Loss: 2.396 | Train Acc: 89.03%\n","\t Val. Loss: 2.781 |  Val. Acc: 50.44% \n","\n","\tTrain Loss: 2.395 | Train Acc: 89.13%\n","\t Val. Loss: 2.775 |  Val. Acc: 51.07% \n","\n","\tTrain Loss: 2.392 | Train Acc: 89.34%\n","\t Val. Loss: 2.776 |  Val. Acc: 51.15% \n","\n","\tTrain Loss: 2.390 | Train Acc: 89.45%\n","\t Val. Loss: 2.775 |  Val. Acc: 51.20% \n","\n","\tTrain Loss: 2.390 | Train Acc: 89.51%\n","\t Val. Loss: 2.777 |  Val. Acc: 50.84% \n","\n","\tTrain Loss: 2.389 | Train Acc: 89.57%\n","\t Val. Loss: 2.779 |  Val. Acc: 50.48% \n","\n","\tTrain Loss: 2.388 | Train Acc: 89.67%\n","\t Val. Loss: 2.775 |  Val. Acc: 50.97% \n","\n","\tTrain Loss: 2.388 | Train Acc: 89.74%\n","\t Val. Loss: 2.775 |  Val. Acc: 51.07% \n","\n","\tTrain Loss: 2.387 | Train Acc: 89.80%\n","\t Val. Loss: 2.770 |  Val. Acc: 51.48% \n","\n","\tTrain Loss: 2.386 | Train Acc: 89.87%\n","\t Val. Loss: 2.769 |  Val. Acc: 51.64% \n","\n","\tTrain Loss: 2.386 | Train Acc: 89.85%\n","\t Val. Loss: 2.770 |  Val. Acc: 51.47% \n","\n","\tTrain Loss: 2.387 | Train Acc: 89.88%\n","\t Val. Loss: 2.769 |  Val. Acc: 51.94% \n","\n","\tTrain Loss: 2.385 | Train Acc: 89.95%\n","\t Val. Loss: 2.769 |  Val. Acc: 51.52% \n","\n","\tTrain Loss: 2.384 | Train Acc: 90.05%\n","\t Val. Loss: 2.768 |  Val. Acc: 51.97% \n","\n","\tTrain Loss: 2.385 | Train Acc: 90.04%\n","\t Val. Loss: 2.768 |  Val. Acc: 51.77% \n","\n","\tTrain Loss: 2.384 | Train Acc: 90.08%\n","\t Val. Loss: 2.767 |  Val. Acc: 51.80% \n","\n","\tTrain Loss: 2.383 | Train Acc: 90.13%\n","\t Val. Loss: 2.765 |  Val. Acc: 51.99% \n","\n","\tTrain Loss: 2.382 | Train Acc: 90.18%\n","\t Val. Loss: 2.766 |  Val. Acc: 51.83% \n","\n","\tTrain Loss: 2.381 | Train Acc: 90.27%\n","\t Val. Loss: 2.767 |  Val. Acc: 51.90% \n","\n","\tTrain Loss: 2.382 | Train Acc: 90.28%\n","\t Val. Loss: 2.768 |  Val. Acc: 51.93% \n","\n","\tTrain Loss: 2.383 | Train Acc: 90.27%\n","\t Val. Loss: 2.769 |  Val. Acc: 51.57% \n","\n","\tTrain Loss: 2.382 | Train Acc: 90.35%\n","\t Val. Loss: 2.766 |  Val. Acc: 52.16% \n","\n","\tTrain Loss: 2.380 | Train Acc: 90.47%\n","\t Val. Loss: 2.765 |  Val. Acc: 52.33% \n","\n","\tTrain Loss: 2.379 | Train Acc: 90.53%\n","\t Val. Loss: 2.766 |  Val. Acc: 52.14% \n","\n","\tTrain Loss: 2.378 | Train Acc: 90.57%\n","\t Val. Loss: 2.768 |  Val. Acc: 51.96% \n","\n","\tTrain Loss: 2.378 | Train Acc: 90.61%\n","\t Val. Loss: 2.766 |  Val. Acc: 51.90% \n","\n","\tTrain Loss: 2.378 | Train Acc: 90.62%\n","\t Val. Loss: 2.767 |  Val. Acc: 51.90% \n","\n","\tTrain Loss: 2.378 | Train Acc: 90.67%\n","\t Val. Loss: 2.764 |  Val. Acc: 52.26% \n","\n","\tTrain Loss: 2.377 | Train Acc: 90.68%\n","\t Val. Loss: 2.766 |  Val. Acc: 51.97% \n","\n","\tTrain Loss: 2.378 | Train Acc: 90.67%\n","\t Val. Loss: 2.761 |  Val. Acc: 52.27% \n","\n","\tTrain Loss: 2.380 | Train Acc: 90.59%\n","\t Val. Loss: 2.761 |  Val. Acc: 52.30% \n","\n","\tTrain Loss: 2.377 | Train Acc: 90.74%\n","\t Val. Loss: 2.762 |  Val. Acc: 52.54% \n","\n","\tTrain Loss: 2.376 | Train Acc: 90.79%\n","\t Val. Loss: 2.761 |  Val. Acc: 52.23% \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LZgzB0ZkHVTI"},"source":["## Model Testing"]},{"cell_type":"code","metadata":{"id":"aZZfnWo0abRx"},"source":["#load weights and tokenizer\n","\n","path='./saved_weights.pt'\n","model.load_state_dict(torch.load(path));\n","model.eval();\n","tokenizer_file = open('./tokenizer.pkl', 'rb')\n","tokenizer = pickle.load(tokenizer_file)\n","\n","#inference \n","\n","import spacy\n","nlp = spacy.load('en')\n","\n","def classify_tweet(tweet):\n","    \n","    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n","    \n","    # tokenize the tweet \n","    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n","    # convert to integer sequence using predefined tokenizer dictionary\n","    indexed = [tokenizer[t] for t in tokenized]        \n","    # compute no. of words        \n","    length = [len(indexed)]\n","    # convert to tensor                                    \n","    tensor = torch.LongTensor(indexed).to(device)   \n","    # reshape in form of batch, no. of words           \n","    tensor = tensor.unsqueeze(1).T  \n","    # convert to tensor                          \n","    length_tensor = torch.LongTensor(length)\n","    # Get the model prediction                  \n","    prediction = model(tensor, length_tensor)\n","\n","    _, pred = torch.max(prediction, 1) \n","    \n","    return categories[pred.item()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTkHLEipIlM9","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1606387277245,"user_tz":-330,"elapsed":1102,"user":{"displayName":"Zoheb Abai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8cnSvE_nYKfBQC_ODazRwdurmZu4zFkLQXksdYW0=s64","userId":"05378514542588697659"}},"outputId":"08f237d8-fbba-4205-f9fe-3188fdd6561d"},"source":["classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Negative'"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"WVjCuKK_LVEF"},"source":["## Discussion on Data Augmentation Techniques \n","\n","You might wonder exactly how you can augment text data. After all, you can’t really flip it horizontally as you can an image! :D \n","\n","In contrast to data augmentation in images, augmentation techniques on data is very specific to final product you are building. As its general usage on any type of textual data doesn't provides a significant performance boost, that's why unlike torchvision, torchtext doesn’t offer a augmentation pipeline. Due to powerful models as transformers, augmentation tecnhiques are not so preferred now-a-days. But its better to know about some techniques with text that will provide your model with a little more information for training. \n","\n","### Synonym Replacement\n","\n","First, you could replace words in the sentence with synonyms, like so:\n","\n","    The dog slept on the mat\n","\n","could become\n","\n","    The dog slept on the rug\n","\n","Aside from the dog's insistence that a rug is much softer than a mat, the meaning of the sentence hasn’t changed. But mat and rug will be mapped to different indices in the vocabulary, so the model will learn that the two sentences map to the same label, and hopefully that there’s a connection between those two words, as everything else in the sentences is the same."]},{"cell_type":"markdown","metadata":{"id":"T_uEfWJpL6Nq"},"source":["### Random Insertion\n","A random insertion technique looks at a sentence and then randomly inserts synonyms of existing non-stopwords into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stopwords (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:\n"]},{"cell_type":"code","metadata":{"id":"7Alm5D7WIvAC","executionInfo":{"status":"ok","timestamp":1622745726431,"user_tz":-330,"elapsed":665,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["def random_insertion(sentence, n): \n","    words = remove_stopwords(sentence) \n","    for _ in range(n):\n","        new_synonym = get_synonyms(random.choice(words))\n","        sentence.insert(randrange(len(sentence)+1), new_synonym) \n","    return sentence"],"execution_count":102,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gqLWzwJ3Mm8h"},"source":["## Random Deletion\n","As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability. Consider of it as pixel dropouts while treating images."]},{"cell_type":"code","metadata":{"id":"-7Dz7JJfMqyC","executionInfo":{"status":"ok","timestamp":1622752516191,"user_tz":-330,"elapsed":660,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["def random_deletion(words, p=0.1): \n","    if len(words) == 1: # return if single word\n","        return words\n","    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n","    if len(remaining) == 0: # if not left, sample a random word\n","        return [random.choice(words)] \n","    else:\n","        return remaining"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9v8kXhcx72oV","executionInfo":{"status":"ok","timestamp":1622751755836,"user_tz":-330,"elapsed":10,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"fcb28358-4146-469a-e664-e01cecabe138"},"source":["### checking random_deletion\n","\n","t =random_deletion([\"I\" ,\"am\", \"just\" ,\"checking\", \"how\", \"this\" ,\"works \"])\n","t"],"execution_count":136,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['am', 'just', 'checking', 'how', 'this', 'works ']"]},"metadata":{"tags":[]},"execution_count":136}]},{"cell_type":"code","metadata":{"id":"hzZmIESWixKb","executionInfo":{"status":"ok","timestamp":1622752513186,"user_tz":-330,"elapsed":3,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["def use_random_deletion(sentence):\n","  word_list = sentence.split()\n","  new_word_list=random_deletion(word_list)\n","  new_sentence = ' '.join(new_word_list)\n","  return new_sentence"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOIbi5WzO5OU"},"source":["### Random Swap\n","The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."]},{"cell_type":"code","metadata":{"id":"1Gkh6y3L71qt","executionInfo":{"status":"ok","timestamp":1622745731527,"user_tz":-330,"elapsed":706,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":[""],"execution_count":104,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnkbG15HO3Yj","executionInfo":{"status":"ok","timestamp":1622752506822,"user_tz":-330,"elapsed":681,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["def random_swap(sentence, n=5): \n","    length = range(len(sentence)) \n","    for _ in range(n):\n","        idx1, idx2 = random.sample(length, 2)\n","        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n","    return sentence"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3n7lOONYECX","executionInfo":{"status":"ok","timestamp":1622752511617,"user_tz":-330,"elapsed":650,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["def use_random_swap(sentence):\n","  word_list = sentence.split()\n","  new_word_list=random_swap(word_list)\n","  new_sentence = ' '.join(new_word_list)\n","  return new_sentence\n","\n","\n"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"599NpwfMR5Vm"},"source":["For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."]},{"cell_type":"markdown","metadata":{"id":"a5aeKuNCRGip"},"source":["### Back Translation\n","\n","Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "]},{"cell_type":"code","metadata":{"id":"pHhNBbYrRXNy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622756632147,"user_tz":-330,"elapsed":2921,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"6ffe2f54-7e7b-43ff-8af0-299672e82d9e"},"source":["!pip install googletrans==3.1.0a0   # this is alpha version, default version was giving error.  error 'NoneType' object has no attribute 'group'\n","import random\n","import googletrans\n","#import googletrans.Translator\n","from googletrans import Translator\n","translator = Translator()\n","\n","\n","def use_back_translation(sentence):\n","        sentence = [sentence]\n","\n","        available_langs = list(googletrans.LANGUAGES.keys()) \n","        trans_lang = random.choice(available_langs) \n","        print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n","\n","        #translations = translator.translate(\"hello\", dest=trans_lang) \n","        translations = translator.translate(sentence,  dest=trans_lang)\n","\n","        t_text = [t.text for t in translations]\n","        print(t_text)\n","\n","        translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n","        en_text = [t.text for t in translations_en_random]\n","        #print(en_text, \" *********************** \")\n","        return(en_text[0])"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n","Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n","Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.22)\n","Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n","Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n","Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n","Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"HQfvuUeGwSgQ","executionInfo":{"status":"ok","timestamp":1622756645996,"user_tz":-330,"elapsed":678,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"aa89de0b-7f1b-475b-beb5-50ae6d722277"},"source":["use_back_translation(\"what is your name?\")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Translating to luxembourgish\n","['what is your name?']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'what is your name?'"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"72DOq_d83ZO4","executionInfo":{"status":"error","timestamp":1622757362602,"user_tz":-330,"elapsed":13469,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"fd5292d4-8568-4369-e1ae-2d54c13b88d7"},"source":["####    Augmenting the dataset\n","\n","for label_class in range(len(Label.vocab)):\n","     df_aug=df[df['labels']==label_class]\n","     old_num_samples=df_aug.shape[0]\n","     print(old_num_samples)\n","     #new_num_samples=600\n","     new_num_samples=1500\n","\n","     if new_num_samples > old_num_samples:\n","        for aug_sample in range(new_num_samples-old_num_samples):\n","              \n","              random_old_sample_no=random.randint(0,old_num_samples-1)\n","              print(random_old_sample_no, \"oopiok\")\n","              \n","              df_new=df_aug.iloc[random_old_sample_no]\n","\n","              #  using random sample\n","              #new_sentence=use_random_swap(df_aug.iloc[random_old_sample_no,0])  \n","              #df_new.iloc[0]=new_sentence\n","\n","              #  using random deletion\n","              #new_sentence=use_random_deletion(df_aug.iloc[random_old_sample_no,0])  \n","              #df_new.iloc[0]=new_sentence\n","\n","              #  using back translation\n","              new_sentence=use_back_translation(df_aug.iloc[random_old_sample_no,0])  \n","              df_new.iloc[0]=new_sentence\n","\n","              df=df.append(df_new,ignore_index = True)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["791 oopiok\n","Translating to japanese\n","['Make like dodge this and one title the .']\n","345 oopiok\n","Translating to hindi\n","['Behind the glitz , Hollywood is sordid and disgusting .']\n","134 oopiok\n","Translating to serbian\n","['An amateurish , quasi-improvised acting exercise shot on ugly digital video .']\n","139 oopiok\n","Translating to sindhi\n","['A limp Eddie Murphy vehicle that even he seems embarrassed to be part of .']\n","3 oopiok\n","Translating to afrikaans\n","[\"Many went to see the attraction for the sole reason that it was hot outside and there was air conditioning inside , and I do n't think that A.C. will help this movie one bit .\"]\n","86 oopiok\n","Translating to kyrgyz\n","[\"... a plotline that 's as lumpy as two-day old porridge ... the filmmakers ' paws , sad to say , were all over this `` un-bear-able '' project !\"]\n","317 oopiok\n","Translating to cebuano\n","['Manipulative claptrap , a period-piece movie-of-the-week , plain old blarney ... take your pick .']\n","168 oopiok\n","Translating to indonesian\n","[\"Godard 's ode to tackling life 's wonderment is a rambling and incoherent manifesto about the vagueness of topical excess ... In Praise of Love remains a ponderous and pretentious endeavor that 's unfocused and tediously exasperating .\"]\n","456 oopiok\n","Translating to hawaiian\n","['Not even the Hanson Brothers can save it']\n","203 oopiok\n","Translating to spanish\n","['Press the delete key .']\n","429 oopiok\n","Translating to swedish\n","['Their parents would do well to cram earplugs in their ears and put pillowcases over their heads for 87 minutes .']\n","606 oopiok\n","Translating to corsican\n","['have to dig deep sink this low .']\n","97 oopiok\n","Translating to sindhi\n","[\"No reason for anyone to invest their hard-earned bucks into a movie which obviously did n't invest much into itself either .\"]\n","121 oopiok\n","Translating to shona\n","[\"The Master Of Disaster - it 's a piece of dreck disguised as comedy .\"]\n","621 oopiok\n","Translating to german\n","[\"Men in Black II achieves ultimate insignificance -- it 's the sci-fi comedy spectacle as Whiffle-Ball epic .\"]\n","132 oopiok\n","Translating to gujarati\n","[\"I can analyze this movie in three words : Thumbs Friggin ' Down .\"]\n","760 oopiok\n","Translating to urdu\n","['There . movie nothing about redeeming is this']\n","700 oopiok\n","Translating to esperanto\n","[\"It made me feel I , and unclean Something the who guy liked There About Pie 's Mary and both American 'm movies .\"]\n","555 oopiok\n","Translating to corsican\n","[\"... if you , like me , think an action film disguised as a war tribute is disgusting to begin with , then you 're in for a painful ride .\"]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-ae1bd6292ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m               \u001b[0;31m#  using back translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m               \u001b[0mnew_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_back_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_old_sample_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m               \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-6eeebd734c8d>\u001b[0m in \u001b[0;36muse_back_translation\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtranslations_en_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0men_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations_en_random\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#print(en_text, \" *********************** \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRANSLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pick_service_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         )\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, files, json, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    599\u001b[0m         )\n\u001b[1;32m    600\u001b[0m         return self.send(\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         response = self.send_handling_redirects(\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         )\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_handling_redirects\u001b[0;34m(self, request, auth, timeout, allow_redirects, history)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             response = self.send_handling_auth(\n\u001b[0;32m--> 648\u001b[0;31m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m             )\n\u001b[1;32m    650\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_handling_auth\u001b[0;34m(self, request, history, auth, timeout)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_response_body\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_single_request\u001b[0;34m(self, request, timeout)\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             )\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 response = connection.request(\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 )\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNewConnectionRequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;34m\"connection.request method=%r url=%r headers=%r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSyncSocketStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mh2_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_streams_semaphore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Receive the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mreason_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reason_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         stream = SyncByteStream(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mreceive_response\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponseReceived\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mwait_for_event\u001b[0;34m(self, stream_id, timeout)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mreceive_events\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mRead\u001b[0m \u001b[0msome\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mH2\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n, timeout)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1054\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gU03oorw-npP","executionInfo":{"status":"ok","timestamp":1622752626142,"user_tz":-330,"elapsed":5,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"32429ea1-5c9c-401b-d263-d9e6b2970dfd"},"source":["df.labels.value_counts()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19    953\n","7     853\n","16    819\n","15    800\n","21    800\n","1     800\n","17    800\n","2     800\n","18    800\n","3     800\n","4     800\n","20    800\n","5     800\n","6     800\n","14    800\n","22    800\n","23    800\n","8     800\n","24    800\n","9     800\n","10    800\n","11    800\n","12    800\n","13    800\n","0     800\n","Name: labels, dtype: int64"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"XmYgRwsn-p3O","executionInfo":{"status":"ok","timestamp":1622752638226,"user_tz":-330,"elapsed":716,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}}},"source":["df.to_pickle(\"data_aug_random_deletion.pkl\")"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"BwBumUCRNnZa","executionInfo":{"status":"ok","timestamp":1622752647760,"user_tz":-330,"elapsed":701,"user":{"displayName":"santosh kumar","photoUrl":"","userId":"05130075998461049044"}},"outputId":"b845bb5e-2986-4d23-88ea-caf53bdc352f"},"source":["df"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Rock is destined to be the 21st Century 's...</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The gorgeously elaborate continuation of `` Th...</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Effective but too-tepid biopic</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>If you sometimes like to go to the movies to h...</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Emerges as something rare , an issue movie tha...</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20220</th>\n","      <td>ever in DiCaprio 's is performance best anythi...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>20221</th>\n","      <td>that as and telling story that examines forbid...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>20222</th>\n","      <td>Jeffrey the 's performance exterminator Tambor...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>20223</th>\n","      <td>Twenty remains years its first release , E.T. ...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>20224</th>\n","      <td>root the fictional . most examination of cause...</td>\n","      <td>24</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20225 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                sentence  labels\n","0      The Rock is destined to be the 21st Century 's...      16\n","1      The gorgeously elaborate continuation of `` Th...      19\n","2                         Effective but too-tepid biopic      12\n","3      If you sometimes like to go to the movies to h...      17\n","4      Emerges as something rare , an issue movie tha...      20\n","...                                                  ...     ...\n","20220  ever in DiCaprio 's is performance best anythi...      24\n","20221  that as and telling story that examines forbid...      24\n","20222  Jeffrey the 's performance exterminator Tambor...      24\n","20223  Twenty remains years its first release , E.T. ...      24\n","20224  root the fictional . most examination of cause...      24\n","\n","[20225 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"zD1OAHH8X7cm"},"source":[""],"execution_count":null,"outputs":[]}]}